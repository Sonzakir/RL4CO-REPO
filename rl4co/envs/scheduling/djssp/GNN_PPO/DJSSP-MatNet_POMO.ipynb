{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Trained vs Untrained Using MatNet and POMO",
   "id": "8fae7c527d77218e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from rl4co.envs.scheduling.djssp.env import DJSSPEnv\n",
    "from rl4co.utils import RL4COTrainer\n",
    "\n",
    "generator_params = {\n",
    "\"num_jobs\" : 8 ,\n",
    "\"num_machines\": 8 ,\n",
    "\"min_processing_time\": 20 ,\n",
    "\"max_processing_time\": 99 ,\n",
    "\"mtbf\": 30,\n",
    "\"mttr\": 10\n",
    "}\n",
    "env = DJSSPEnv(generator_params=generator_params,\n",
    "_torchrl_mode=True,\n",
    "stepwise_reward=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "dd7e46d76b8c7cef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device Related Details",
   "id": "7cc65054ac5d0338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    batch_size = 256\n",
    "    train_data_size = 2_000\n",
    "    embed_dim = 128\n",
    "    num_encoder_layers = 4\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    batch_size = 32\n",
    "    train_data_size = 1_000\n",
    "    embed_dim = 64\n",
    "    num_encoder_layers = 2"
   ],
   "id": "fb7f3995d1a513ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### WandB\n",
   "id": "435daeac55c58675"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "run = wandb.init(\n",
    "    project = \"rl4co\",\n",
    "    notes = \"MatNET-POMO-Training\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = WandbLogger(project=\"rl4co\", name=\"MatNET-POMO-Training\")"
   ],
   "id": "64814e3490f28f0d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy = L2DPolicy",
   "id": "4eceeee7f9afb87b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from rl4co.models.nn.env_embeddings.init import FJSPMatNetInitEmbedding\n",
    "from rl4co.models.zoo.matnet.matnet_w_sa import Encoder\n",
    "from rl4co.models import L2DPolicy, POMO\n",
    "\n",
    "init_embedding=FJSPMatNetInitEmbedding(embed_dim=256,scaling_factor=1000)\n",
    "encoder = Encoder(embed_dim=256,num_heads=8,num_layers=4,normalization=\"batch\",init_embedding=init_embedding)\n",
    "policy = L2DPolicy(env_name=env.name,encoder=encoder,embed_dim=256,stepwise_encoding=False,het_emb=True,scaling_factor=1000)\n",
    "\n"
   ],
   "id": "c78a7f75aee40ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model = POMO",
   "id": "d60361a2eeb7927e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = {\n",
    "  \"val\": [\"reward\", \"max_reward\"],\n",
    "  \"test\":[\"reward\", \"max_reward\"] }\n",
    "\n",
    "model = POMO(env=env,policy=policy,batch_size=64,num_starts=10,num_augment=0,baseline=\"shared\",metrics=metrics)"
   ],
   "id": "55b3a2e455596622"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scheduling of the Untrained Model",
   "id": "2d13004729b4c79b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "td_init = env.reset(batch_size=[1])\n",
    "# we must send td to device otherwise it throws an error\n",
    "td_init = td_init.to(device)"
   ],
   "id": "e47039e82c7e1ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# same for the policy\n",
    "policy = model.policy.to(device=device)"
   ],
   "id": "14e271b81d5acc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Greedy\n",
   "id": "782df22d80205407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_untrained_greedy = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\")\n",
    "\n",
    "print(\"Untrained greedy output with the makespan \", out_untrained_greedy[\"reward\"][0])\n",
    "\n",
    "# for multiple batches use this:\n",
    "# print(f\"Greedy - Rewards: {[f'{-r.item():.2f}' for r in out_untrained_greedy['reward']]}\")\n"
   ],
   "id": "fbee244905927959"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# render the output of the untrained greedy\n",
    "env.render(out_untrained_greedy[\"td\"] , 0)"
   ],
   "id": "1f195e8302e497bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MultistartSampling",
   "id": "5e1b722c396660e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_untrained_multistart = policy.generate(td_init.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "print(f\"Untrained Multistart - Makespan: {[f'{-r.item():.2f}' for r in out_untrained_multistart['reward']]}\")"
   ],
   "id": "f8f4e8743db715a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "env.render(out_untrained_multistart[\"td\"] , 0)",
   "id": "4692d72c460613b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the Model",
   "id": "efa5b492b2725f84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checkpoint - Callback Setup",
   "id": "848cf5c6b186f0bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "\n",
    "# Checkpointing callback: save models when validation reward improves\n",
    "checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/\n",
    "                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n",
    "                                        save_top_k=1, # save only the best model\n",
    "                                        save_last=True, # save the last model\n",
    "                                        monitor=\"val/reward\", # monitor validation reward\n",
    "                                        mode=\"max\") # maximize validation reward\n",
    "# Print model summary\n",
    "rich_model_summary = RichModelSummary(max_depth=3)\n",
    "\n",
    "# Callbacks list\n",
    "callbacks = [checkpoint_callback, rich_model_summary]\n",
    "\n"
   ],
   "id": "7020d7df5c425cca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "3dd66c273e95b5c0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#CHECKPOINT_PATH = \"last.ckpt\"\n",
    "CHECKPOINT_PATH = \"checkpoints/last.ckpt\"\n",
    "try:\n",
    "    model = POMO.load_from_checkpoint(CHECKPOINT_PATH)\n",
    "except FileNotFoundError:\n",
    "\n",
    "    # max_epochs = 10\n",
    "    trainer = RL4COTrainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "finally:\n",
    "    model = model.to(device)\n"
   ],
   "id": "41b2f957927c2957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scheduling of the Trained Model",
   "id": "500e52e2f994e796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "td_init = td_init.to(device)\n",
    "# same for the policy\n",
    "policy = model.policy.to(device=device)"
   ],
   "id": "2be16da671175627"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Greedy\n",
   "id": "442cb87c11873896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_trained_greedy = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\")\n",
    "\n",
    "print(\"Trained greedy output with the makespan \", out_trained_greedy[\"reward\"][0])"
   ],
   "id": "9d3d5f5d8ab17bf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "env.render(out_trained_greedy[\"td\"] , 0)",
   "id": "f7a99a3fddc4f7b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multistart\n",
   "id": "75921bf594807d85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_trained_multistart = policy.generate(td_init.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "print(f\"Trained Multistart - Rewards: {[f'{-r.item():.2f}' for r in out_trained_multistart['reward']]}\")\n"
   ],
   "id": "e0ce21d1bc281df5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "env.render(out_trained_multistart[\"td\"] , 0)",
   "id": "21e409416b8e050"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on Taillard Benchmark",
   "id": "ac851c57cce49816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! git clone https://github.com/tamy0612/JSPLIB.git",
   "id": "174e1ce4e8a79d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def prepare_taillard_data(nj, nm):\n",
    "    # Target folder for Taillard instances\n",
    "    fp = f\"taillard/{nj}x{nm}\"\n",
    "\n",
    "    if not os.path.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open('JSPLIB/instances.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter Taillard instances with matching jobs and machines\n",
    "    instances = [x for x in data if \"ta\" in x[\"name\"] and x[\"jobs\"] == nj and x[\"machines\"] == nm]\n",
    "    print(f\"Found {len(instances)} instances for {nj} jobs and {nm} machines\")\n",
    "\n",
    "    if not instances:\n",
    "        raise FileNotFoundError(f\"No matching Taillard instances found for {nj}x{nm}\")\n",
    "\n",
    "    # Copy files and validate\n",
    "    for instance in instances:\n",
    "        source_path = os.path.join(\"JSPLIB\", instance['path'])\n",
    "        target_path = os.path.join(fp, f\"{instance['name']}.txt\")\n",
    "\n",
    "        # Check if the source file exists\n",
    "        if os.path.exists(source_path):\n",
    "            print(f\"Copying {source_path} to {target_path}\")\n",
    "            os.system(f\"cp {source_path} {target_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Source file {source_path} does not exist\")\n",
    "\n",
    "    # Verify if files were copied\n",
    "    files_in_target = os.listdir(fp)\n",
    "    assert len(files_in_target) > 0, f\"No files copied to {fp}. Check source paths.\"\n",
    "    print(f\"Successfully prepared {len(files_in_target)} files in {fp}\")"
   ],
   "id": "9032e8ad3efbeca5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# path to taillard instances\n",
    "FILE_PATH = \"taillard/{nj}x{nm}\"\n",
    "\n",
    "results = {}\n",
    "instance_types = [(15, 15), (20, 15), (20, 20), (30, 15), (30, 20)]\n",
    "\n",
    "for instance_type in instance_types:\n",
    "    print(\"------------\")\n",
    "    nj, nm = instance_type\n",
    "    prepare_taillard_data(nj, nm)\n",
    "    dataset = env.dataset(batch_size=[10], phase=\"test\", filename=FILE_PATH.format(nj=nj, nm=nm))\n",
    "    dl = DataLoader(dataset, batch_size=5, collate_fn=dataset.collate_fn)\n",
    "    rewards = []\n",
    "\n",
    "    for batch in dl:\n",
    "        td = env.reset(batch).to(device)\n",
    "        # use policy.generate to avoid grad calculations which can lead to oom\n",
    "        out = model.policy.generate(td, env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "        rewards.append(out[\"reward\"])\n",
    "\n",
    "    reward = torch.cat(rewards, dim=0).mean().item()\n",
    "    results[instance_type] = reward\n",
    "\n",
    "    print(\"Done evaluating instance type %s with reward %s\" % (instance_type, reward))\n",
    "\n",
    "    # avoid ooms due to cache not being cleared\n",
    "    model.rb.empty()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "f0c23671e03b2fe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "wandb.finish()",
   "id": "76730df0a9e86ebc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
