{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaed368b-b463-4429-8fe3-afa9fd966056",
   "metadata": {},
   "source": "## AttentionModel - POMO"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from rl4co.envs.scheduling.jssp.env import JSSPEnv\n",
    "from rl4co.models import  POMO\n",
    "from rl4co.envs.scheduling.djssp.env import DJSSPEnv\n",
    "from rl4co.models import  L2DAttnPolicy\n",
    "from rl4co.utils import RL4COTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "generator_params = {\n",
    "\"num_jobs\" : 8 ,\n",
    "\"num_machines\": 8 ,\n",
    "\"min_processing_time\": 20 ,\n",
    "\"max_processing_time\": 99 ,\n",
    "\"mtbf\": 30,\n",
    "\"mttr\": 10\n",
    "}\n",
    "env = DJSSPEnv(generator_params=generator_params,\n",
    "_torchrl_mode=True,\n",
    "stepwise_reward=True\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device-related details",
   "id": "7417549731c03487"
  },
  {
   "cell_type": "code",
   "id": "c6fe40b5acd44f8",
   "metadata": {},
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    batch_size = 256\n",
    "    train_data_size = 2_000\n",
    "    embed_dim = 128\n",
    "    num_encoder_layers = 4\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    batch_size = 32\n",
    "    train_data_size = 1_000\n",
    "    embed_dim = 64\n",
    "    num_encoder_layers = 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### WandB",
   "id": "b3c3c49ba7596aa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "run = wandb.init(\n",
    "    project = \"rl4co\",\n",
    "    notes = \"GNN-POMO-Training\"\n",
    ")\n",
    "\n",
    "\n",
    "logger = WandbLogger(project=\"rl4co\", name=\"AM-POMO-Training\")"
   ],
   "id": "2490c97154c71ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Policy = L2DAttentionPolicy",
   "id": "a98202d9b40732e9"
  },
  {
   "cell_type": "code",
   "id": "11d916708551f353",
   "metadata": {},
   "source": "policy = L2DAttnPolicy(env_name=env.name, scaling_factor=1000, normalization=\"batch\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model = POMO",
   "id": "87d04e95797b125a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "metrics = {\n",
    "    \"val\": [\"reward\", \"max_reward\"] ,\n",
    "    \"test\": [\"reward\", \"max_reward\"]\n",
    "}\n",
    "model = POMO(env, policy = policy, batch_size=64, num_starts=10 , num_augment=0 , baseline=\"shared\",metrics=metrics)"
   ],
   "id": "2013cd6e475379dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scheduling of the Untrained Model",
   "id": "12579496c0f67d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "td_init = env.reset(batch_size=[1])\n",
    "# we must send td to device otherwise it throws an error\n",
    "td_init = td_init.to(device)"
   ],
   "id": "8887565f45282dd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# same for the policy\n",
    "policy = model.policy.to(device=device)"
   ],
   "id": "60d820bf0d8e0d29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Greedy",
   "id": "f9e50ab7964e7b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- L2DAttentionPolicy_POMO cannot be used with a greedy strategy in scheduling problems.",
   "id": "8d0d40401326ff3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# out_untrained_greedy = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\")\n",
    "\n",
    "# print(\"Untrained greedy output with the makespan \", out_untrained_greedy[\"reward\"][0])\n",
    "\n",
    "# for multiple batches use this:\n",
    "# print(f\"Greedy - Rewards: {[f'{-r.item():.2f}' for r in out_untrained_greedy['reward']]}\")"
   ],
   "id": "474a087a8c2e32ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# render the output of the untrained greedy\n",
    "# env.render(out_untrained_greedy[\"td\"] , 0)"
   ],
   "id": "9776710e42df63d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MultistartSampling",
   "id": "f8649f558f961fb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out_untrained_multistart = policy(td_init.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "print(f\"Untrained Multistart - Makespan: {[f'{-r.item():.2f}' for r in out_untrained_multistart['reward']]}\")"
   ],
   "id": "18bddf85f2eab1ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.render(out_untrained_multistart[\"td\"] , 0)\n",
   "id": "8a8645451f68be13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the Model",
   "id": "737f40ddee0af051"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checkpoint - Callback Setup",
   "id": "1696ae7a900de1e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint, RichModelSummary\n",
    "\n",
    "# Checkpointing callback: save models when validation reward improves\n",
    "checkpoint_callback = ModelCheckpoint(  dirpath=\"checkpoints\", # save to checkpoints/\n",
    "                                        filename=\"epoch_{epoch:03d}\",  # save as epoch_XXX.ckpt\n",
    "                                        save_top_k=1, # save only the best model\n",
    "                                        save_last=True, # save the last model\n",
    "                                        monitor=\"val/reward\", # monitor validation reward\n",
    "                                        mode=\"max\") # maximize validation reward\n",
    "# Print model summary\n",
    "rich_model_summary = RichModelSummary(max_depth=3)\n",
    "\n",
    "# Callbacks list\n",
    "callbacks = [checkpoint_callback, rich_model_summary]\n",
    "\n"
   ],
   "id": "3e303769fd2b3223",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "caea66b5a7669edb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# max_epochs=25,\n",
    "\n",
    "#CHECKPOINT_PATH = \"last.ckpt\"\n",
    "CHECKPOINT_PATH = \"checkpoints/last.ckpt\"\n",
    "try:\n",
    "    model = POMO.load_from_checkpoint(CHECKPOINT_PATH)\n",
    "except FileNotFoundError:\n",
    "\n",
    "    # max_epochs = 10\n",
    "    trainer = RL4COTrainer(\n",
    "    max_epochs=25,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    trainer.fit(model)\n",
    "finally:\n",
    "    model = model.to(device)"
   ],
   "id": "d49676aa05a9531a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:93: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 6\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 6\u001B[0m     model \u001B[38;5;241m=\u001B[39m POMO\u001B[38;5;241m.\u001B[39mload_from_checkpoint(CHECKPOINT_PATH)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# max_epochs = 10\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\reinforce\\reinforce.py:189\u001B[0m, in \u001B[0;36mREINFORCE.load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, load_baseline, **kwargs)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# Do not use strict\u001B[39;00m\n\u001B[1;32m--> 189\u001B[0m loaded \u001B[38;5;241m=\u001B[39m _load_from_checkpoint(\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m    191\u001B[0m     checkpoint_path,\n\u001B[0;32m    192\u001B[0m     map_location,\n\u001B[0;32m    193\u001B[0m     hparams_file,\n\u001B[0;32m    194\u001B[0m     strict,\n\u001B[0;32m    195\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    196\u001B[0m )\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# Load baseline state dict\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\core\\saving.py:63\u001B[0m, in \u001B[0;36m_load_from_checkpoint\u001B[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pl_legacy_patch():\n\u001B[1;32m---> 63\u001B[0m     checkpoint \u001B[38;5;241m=\u001B[39m pl_load(checkpoint_path, map_location\u001B[38;5;241m=\u001B[39mmap_location)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# convert legacy checkpoints to the new format\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\fabric\\utilities\\cloud_io.py:59\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(path_or_url, map_location, weights_only)\u001B[0m\n\u001B[0;32m     58\u001B[0m fs \u001B[38;5;241m=\u001B[39m get_filesystem(path_or_url)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m fs\u001B[38;5;241m.\u001B[39mopen(path_or_url, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mload(\n\u001B[0;32m     61\u001B[0m         f,\n\u001B[0;32m     62\u001B[0m         map_location\u001B[38;5;241m=\u001B[39mmap_location,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m     63\u001B[0m         weights_only\u001B[38;5;241m=\u001B[39mweights_only,\n\u001B[0;32m     64\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\fsspec\\spec.py:1293\u001B[0m, in \u001B[0;36mAbstractFileSystem.open\u001B[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001B[0m\n\u001B[0;32m   1292\u001B[0m ac \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautocommit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_intrans)\n\u001B[1;32m-> 1293\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open(\n\u001B[0;32m   1294\u001B[0m     path,\n\u001B[0;32m   1295\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m   1296\u001B[0m     block_size\u001B[38;5;241m=\u001B[39mblock_size,\n\u001B[0;32m   1297\u001B[0m     autocommit\u001B[38;5;241m=\u001B[39mac,\n\u001B[0;32m   1298\u001B[0m     cache_options\u001B[38;5;241m=\u001B[39mcache_options,\n\u001B[0;32m   1299\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1300\u001B[0m )\n\u001B[0;32m   1301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\fsspec\\implementations\\local.py:184\u001B[0m, in \u001B[0;36mLocalFileSystem._open\u001B[1;34m(self, path, mode, block_size, **kwargs)\u001B[0m\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmakedirs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parent(path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 184\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LocalFileOpener(path, mode, fs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\fsspec\\implementations\\local.py:306\u001B[0m, in \u001B[0;36mLocalFileOpener.__init__\u001B[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocksize \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mDEFAULT_BUFFER_SIZE\n\u001B[1;32m--> 306\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_open()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\fsspec\\implementations\\local.py:311\u001B[0m, in \u001B[0;36mLocalFileOpener._open\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocommit \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m--> 311\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode)\n\u001B[0;32m    312\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:/Users/soner/Desktop/rl4co/rl4co/envs/scheduling/djssp/GNN_PPO/checkpoints/last.ckpt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    571\u001B[0m     ckpt_path,\n\u001B[0;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    574\u001B[0m )\n\u001B[1;32m--> 575\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(model, ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:944\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    942\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mprepare_data()\n\u001B[1;32m--> 944\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_setup_hook(\u001B[38;5;28mself\u001B[39m)  \u001B[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001B[39;00m\n\u001B[0;32m    945\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: configuring model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:104\u001B[0m, in \u001B[0;36m_call_setup_hook\u001B[1;34m(trainer)\u001B[0m\n\u001B[0;32m    103\u001B[0m _call_callback_hooks(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup\u001B[39m\u001B[38;5;124m\"\u001B[39m, stage\u001B[38;5;241m=\u001B[39mfn)\n\u001B[1;32m--> 104\u001B[0m _call_lightning_module_hook(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup\u001B[39m\u001B[38;5;124m\"\u001B[39m, stage\u001B[38;5;241m=\u001B[39mfn)\n\u001B[0;32m    106\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mbarrier(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_setup\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:171\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 171\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\common\\base.py:147\u001B[0m, in \u001B[0;36mRL4COLitModule.setup\u001B[1;34m(self, stage)\u001B[0m\n\u001B[0;32m    145\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting up datasets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_dataset(\n\u001B[1;32m--> 147\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mdataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_data_size\u001B[39m\u001B[38;5;124m\"\u001B[39m], phase\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    148\u001B[0m )\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mdataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_data_size\u001B[39m\u001B[38;5;124m\"\u001B[39m], phase\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\common\\base.py:246\u001B[0m, in \u001B[0;36mRL4COEnvBase.dataset\u001B[1;34m(self, batch_size, phase, filename)\u001B[0m\n\u001B[0;32m    245\u001B[0m         log\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mphase\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_file not set. Generating dataset instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 246\u001B[0m     td \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator(batch_size)\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\common\\utils.py:27\u001B[0m, in \u001B[0;36mGenerator.__call__\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m     26\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m [batch_size] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch_size, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m batch_size\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(batch_size)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:187\u001B[0m, in \u001B[0;36mDJSSPGenerator._generate\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m    186\u001B[0m td_clone \u001B[38;5;241m=\u001B[39m proc_times\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m--> 187\u001B[0m actual_proc_times \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulate_actual_processing_times(td\u001B[38;5;241m=\u001B[39mtd_clone)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# machine breakdowns\u001B[39;00m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# simulate machine breakdowns for given mtbf and mttr\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# list index = machine_idx; TIME-DURATION\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;66;03m# to undertstand the structure -> jupyter notebook Generator\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:143\u001B[0m, in \u001B[0;36mDJSSPGenerator._simulate_actual_processing_times\u001B[1;34m(self, td)\u001B[0m\n\u001B[0;32m    142\u001B[0m stochastic_noise \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mnormal(\u001B[38;5;241m0\u001B[39m, variance \u001B[38;5;241m*\u001B[39m td[i, j, z])\n\u001B[1;32m--> 143\u001B[0m actual_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, td[i, j, z] \u001B[38;5;241m+\u001B[39m stochastic_noise)\n\u001B[0;32m    144\u001B[0m td[i, j, z] \u001B[38;5;241m=\u001B[39m actual_time\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 18\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m     \u001B[38;5;66;03m# max_epochs = 10\u001B[39;00m\n\u001B[0;32m     10\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m RL4COTrainer(\n\u001B[0;32m     11\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m25\u001B[39m,\n\u001B[0;32m     12\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39maccelerator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcallbacks\n\u001B[0;32m     16\u001B[0m     )\n\u001B[1;32m---> 18\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mfit(model)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     20\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\utils\\trainer.py:146\u001B[0m, in \u001B[0;36mRL4COTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    141\u001B[0m         log\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    142\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverriding gradient_clip_val to None for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautomatic_optimization=False\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m models\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    143\u001B[0m         )\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_clip_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 146\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m    147\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m    148\u001B[0m     train_dataloaders\u001B[38;5;241m=\u001B[39mtrain_dataloaders,\n\u001B[0;32m    149\u001B[0m     val_dataloaders\u001B[38;5;241m=\u001B[39mval_dataloaders,\n\u001B[0;32m    150\u001B[0m     datamodule\u001B[38;5;241m=\u001B[39mdatamodule,\n\u001B[0;32m    151\u001B[0m     ckpt_path\u001B[38;5;241m=\u001B[39mckpt_path,\n\u001B[0;32m    152\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 539\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_and_handle_interrupt(\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001B[0;32m    541\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[0;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[1;32m---> 64\u001B[0m     exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scheduling of the Trained Model",
   "id": "272843c0fc97d414"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "td_init = td_init.to(device)\n",
    "# same for the policy\n",
    "policy = model.policy.to(device=device)"
   ],
   "id": "5912441156addddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Greedy",
   "id": "883561d4d64c2bef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# out_trained_greedy = policy(td_init.clone(), env, phase=\"test\", decode_type=\"greedy\")\n",
    "\n",
    "# print(\"Trained greedy output with the makespan \", out_trained_greedy[\"reward\"][0])"
   ],
   "id": "2de6441a4ad7fbc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# env.render(out_trained_greedy[\"td\"] , 0)",
   "id": "fdc0c8fd8f69e52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multistart",
   "id": "459f11c312e00c90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out_trained_multistart = policy(td_init.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "print(f\"Trained Multistart - Rewards: {[f'{-r.item():.2f}' for r in out_trained_multistart['reward']]}\")\n"
   ],
   "id": "de3f15ec024e03e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.render(out_trained_multistart[\"td\"] , 0)\n",
   "id": "f8758262236c75e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on Taillard Benchmark",
   "id": "a02335afcd956a61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! git clone https://github.com/tamy0612/JSPLIB.git\n",
   "id": "c2691c0258d058a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def prepare_taillard_data(nj, nm):\n",
    "    # Target folder for Taillard instances\n",
    "    fp = f\"taillard/{nj}x{nm}\"\n",
    "\n",
    "    if not os.path.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open('JSPLIB/instances.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter Taillard instances with matching jobs and machines\n",
    "    instances = [x for x in data if \"ta\" in x[\"name\"] and x[\"jobs\"] == nj and x[\"machines\"] == nm]\n",
    "    print(f\"Found {len(instances)} instances for {nj} jobs and {nm} machines\")\n",
    "\n",
    "    if not instances:\n",
    "        raise FileNotFoundError(f\"No matching Taillard instances found for {nj}x{nm}\")\n",
    "\n",
    "    # Copy files and validate\n",
    "    for instance in instances:\n",
    "        source_path = os.path.join(\"JSPLIB\", instance['path'])\n",
    "        target_path = os.path.join(fp, f\"{instance['name']}.txt\")\n",
    "\n",
    "        # Check if the source file exists\n",
    "        if os.path.exists(source_path):\n",
    "            print(f\"Copying {source_path} to {target_path}\")\n",
    "            os.system(f\"cp {source_path} {target_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Source file {source_path} does not exist\")\n",
    "\n",
    "    # Verify if files were copied\n",
    "    files_in_target = os.listdir(fp)\n",
    "    assert len(files_in_target) > 0, f\"No files copied to {fp}. Check source paths.\"\n",
    "    print(f\"Successfully prepared {len(files_in_target)} files in {fp}\")"
   ],
   "id": "9c9e117b8c1b2ad0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# path to taillard instances\n",
    "FILE_PATH = \"taillard/{nj}x{nm}\"\n",
    "\n",
    "results = {}\n",
    "instance_types = [(15, 15), (20, 15), (20, 20), (30, 15), (30, 20)]\n",
    "\n",
    "for instance_type in instance_types:\n",
    "    print(\"------------\")\n",
    "    nj, nm = instance_type\n",
    "    prepare_taillard_data(nj, nm)\n",
    "    dataset = env.dataset(batch_size=[10], phase=\"test\", filename=FILE_PATH.format(nj=nj, nm=nm))\n",
    "    dl = DataLoader(dataset, batch_size=5, collate_fn=dataset.collate_fn)\n",
    "    rewards = []\n",
    "\n",
    "    for batch in dl:\n",
    "        td = env.reset(batch).to(device)\n",
    "        # use policy.generate to avoid grad calculations which can lead to oom\n",
    "        out = model.policy.generate(td, env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "        rewards.append(out[\"reward\"])\n",
    "\n",
    "    reward = torch.cat(rewards, dim=0).mean().item()\n",
    "    results[instance_type] = reward\n",
    "\n",
    "    print(\"Done evaluating instance type %s with reward %s\" % (instance_type, reward))\n",
    "\n",
    "    # avoid ooms due to cache not being cleared\n",
    "    model.rb.empty()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "c013b1dcb83c5905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "bd272df40b53aef1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
