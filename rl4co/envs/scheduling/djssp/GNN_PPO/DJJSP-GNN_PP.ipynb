{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Trained vs Untrained using Graph Neural Network and PPO Model\n",
    "- Model:L2DPPOModel"
   ],
   "id": "ef8c1cc8bce0a1c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-24T16:51:41.746309Z",
     "start_time": "2024-12-24T16:51:34.491945Z"
    }
   },
   "source": [
    "import time\n",
    "import torch\n",
    "from rl4co.envs.scheduling.djssp.env import DJSSPEnv\n",
    "from rl4co.envs.scheduling.jssp.env import JSSPEnv\n",
    "from rl4co.models import L2DPolicy, AttentionModel, L2DAttnPolicy\n",
    "from rl4co.utils import RL4COTrainer\n",
    "import gc\n",
    "from rl4co.envs import JSSPEnv, FJSPEnv\n",
    "from rl4co.models.zoo.l2d.model import L2DPPOModel\n",
    "from rl4co.models.zoo.l2d.policy import L2DPolicy4PPO\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "generator_params = {\n",
    "\"num_jobs\" : 8 ,\n",
    "\"num_machines\": 8 ,\n",
    "\"min_processing_time\": 1 ,\n",
    "\"max_processing_time\": 99 ,\n",
    "\"mtbf\":20 ,\n",
    "\"mttr\":4\n",
    "}\n",
    "env = DJSSPEnv(generator_params=generator_params,\n",
    "_torchrl_mode=True,\n",
    "stepwise_reward=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:51:51.746591Z",
     "start_time": "2024-12-24T16:51:51.509223Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "17eab6269df66a70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T16:57:51.770936Z",
     "start_time": "2024-12-24T16:57:51.392467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "policy_kwargs = {\n",
    "    \"embed_dim\": 256 ,\n",
    "    \"num_encoder_layers\": 3,\n",
    "    \"scaling_factor\": 1000,\n",
    "    \"ppo_epochs\": 2,\n",
    "    \"het_emb\": False,\n",
    "    \"normalization\": \"instance\",\n",
    "    \"test_decode_type\": \"greedy\"\n",
    "}\n",
    "\n",
    "model = L2DPPOModel(env=env, policy_kwargs=policy_kwargs ,batch_size=128,val_batch_size=512,test_batch_size=64,mini_batch_size=512)"
   ],
   "id": "1145f4f9458c4786",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\Desktop\\rl4co\\rl4co\\models\\zoo\\l2d\\policy.py:48: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  log.warn(f\"Unused kwargs: {constructive_policy_kw}\")\n",
      "Unused kwargs: {'ppo_epochs': 2}\n",
      "Found 1 unused kwargs: {'ppo_epochs': 2}\n",
      "C:\\Users\\soner\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "C:\\Users\\soner\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scheduling of the `untrained Model`\n",
    "- burada normalde\n",
    "    `td = env.reset(batch_size=[1])\n",
    "policy = model.policy.to(\"cpu\")\n",
    "out = model.policy.generate(td.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)` kullaniyordum, lakin device'daki sikinti belki bundandir diye policy'i device gÃ¶nderiyorum"
   ],
   "id": "f7efc27841361595"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "td = env.reset(batch_size=[1])\n",
    "policy = model.policy.to(device)\n",
    "out = model.policy.generate(td.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)"
   ],
   "id": "e39af82e62c8e46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training the model",
   "id": "20756e04ecfc6d01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T17:21:13.480249Z",
     "start_time": "2024-12-24T17:06:27.009686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    accelerator=\"gpu\"\n",
    "else:\n",
    "    accelerator=\"cpu\"\n",
    "\n",
    "# max_epochs = 10\n",
    "trainer = RL4COTrainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "model = model.to(device)"
   ],
   "id": "1ea982111c6067d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\soner\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Overriding gradient_clip_val to None for 'automatic_optimization=False' models\n",
      "C:\\Users\\soner\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:45: Attribute 'policy' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['policy'])`.\n",
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "C:\\Users\\soner\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:93: SyntaxWarning: invalid escape sequence '\\g'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    571\u001B[0m     ckpt_path,\n\u001B[0;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    574\u001B[0m )\n\u001B[1;32m--> 575\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:944\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    942\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39mprepare_data()\n\u001B[1;32m--> 944\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_setup_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# allow user to set up LightningModule in accelerator environment\u001B[39;00m\n\u001B[0;32m    945\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: configuring model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:104\u001B[0m, in \u001B[0;36m_call_setup_hook\u001B[1;34m(trainer)\u001B[0m\n\u001B[0;32m    103\u001B[0m _call_callback_hooks(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup\u001B[39m\u001B[38;5;124m\"\u001B[39m, stage\u001B[38;5;241m=\u001B[39mfn)\n\u001B[1;32m--> 104\u001B[0m \u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msetup\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mbarrier(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost_setup\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:171\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 171\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\common\\base.py:147\u001B[0m, in \u001B[0;36mRL4COLitModule.setup\u001B[1;34m(self, stage)\u001B[0m\n\u001B[0;32m    145\u001B[0m log\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting up datasets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_dataset(\n\u001B[1;32m--> 147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_cfg\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain_data_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    148\u001B[0m )\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mdataset(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_data_size\u001B[39m\u001B[38;5;124m\"\u001B[39m], phase\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\common\\base.py:246\u001B[0m, in \u001B[0;36mRL4COEnvBase.dataset\u001B[1;34m(self, batch_size, phase, filename)\u001B[0m\n\u001B[0;32m    245\u001B[0m         log\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mphase\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_file not set. Generating dataset instead\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 246\u001B[0m     td \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\common\\utils.py:27\u001B[0m, in \u001B[0;36mGenerator.__call__\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m     26\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m [batch_size] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch_size, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m batch_size\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:187\u001B[0m, in \u001B[0;36mDJSSPGenerator._generate\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m    186\u001B[0m td_clone \u001B[38;5;241m=\u001B[39m proc_times\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m--> 187\u001B[0m actual_proc_times \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_simulate_actual_processing_times\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtd_clone\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;66;03m# machine breakdowns\u001B[39;00m\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# simulate machine breakdowns for given mtbf and mttr\u001B[39;00m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# list index = machine_idx; TIME-DURATION\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;66;03m# to undertstand the structure -> jupyter notebook Generator\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\envs\\scheduling\\djssp\\generator.py:142\u001B[0m, in \u001B[0;36mDJSSPGenerator._simulate_actual_processing_times\u001B[1;34m(self, td)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m z \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(td\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]):\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;66;03m# here: Normal Distribution\u001B[39;00m\n\u001B[1;32m--> 142\u001B[0m     stochastic_noise \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariance\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtd\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m     actual_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m0\u001B[39m, td[i, j, z] \u001B[38;5;241m+\u001B[39m stochastic_noise)\n",
      "File \u001B[1;32mnumpy\\\\random\\\\mtrand.pyx:1557\u001B[0m, in \u001B[0;36mnumpy.random.mtrand.RandomState.normal\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_common.pyx:588\u001B[0m, in \u001B[0;36mnumpy.random._common.cont\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\torch\\_tensor.py:1145\u001B[0m, in \u001B[0;36mTensor.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m   1143\u001B[0m __array_priority__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m  \u001B[38;5;66;03m# prefer Tensor ops over numpy ones\u001B[39;00m\n\u001B[1;32m-> 1145\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1146\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# max_epochs = 10\u001B[39;00m\n\u001B[0;32m      2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m RL4COTrainer(\n\u001B[0;32m      3\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      4\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      5\u001B[0m     devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      6\u001B[0m     logger\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 9\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\utils\\trainer.py:146\u001B[0m, in \u001B[0;36mRL4COTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    141\u001B[0m         log\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    142\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverriding gradient_clip_val to None for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautomatic_optimization=False\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m models\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    143\u001B[0m         )\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_clip_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 146\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 539\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\reinforce\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[0;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[1;32m---> 64\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[0;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scheduling performance of the trained model",
   "id": "55c0e37289470b1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "out = model.policy.generate(td.clone(), env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)",
   "id": "3547cc069bcfd973"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exact finishing time",
   "id": "9db286434e3e909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "out[\"reward\"]",
   "id": "7534dfdc2b326650"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on Taillard Benchmark",
   "id": "232e64f98fda8a8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! git clone https://github.com/tamy0612/JSPLIB.git\n",
   "id": "7fd53cd381482bec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def prepare_taillard_data(nj, nm):\n",
    "    # Target folder for Taillard instances\n",
    "    fp = f\"taillard/{nj}x{nm}\"\n",
    "\n",
    "    if not os.path.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open('JSPLIB/instances.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter Taillard instances with matching jobs and machines\n",
    "    instances = [x for x in data if \"ta\" in x[\"name\"] and x[\"jobs\"] == nj and x[\"machines\"] == nm]\n",
    "    print(f\"Found {len(instances)} instances for {nj} jobs and {nm} machines\")\n",
    "\n",
    "    if not instances:\n",
    "        raise FileNotFoundError(f\"No matching Taillard instances found for {nj}x{nm}\")\n",
    "\n",
    "    # Copy files and validate\n",
    "    for instance in instances:\n",
    "        source_path = os.path.join(\"JSPLIB\", instance['path'])\n",
    "        target_path = os.path.join(fp, f\"{instance['name']}.txt\")\n",
    "\n",
    "        # Check if the source file exists\n",
    "        if os.path.exists(source_path):\n",
    "            print(f\"Copying {source_path} to {target_path}\")\n",
    "            os.system(f\"cp {source_path} {target_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Source file {source_path} does not exist\")\n",
    "\n",
    "    # Verify if files were copied\n",
    "    files_in_target = os.listdir(fp)\n",
    "    assert len(files_in_target) > 0, f\"No files copied to {fp}. Check source paths.\"\n",
    "    print(f\"Successfully prepared {len(files_in_target)} files in {fp}\")"
   ],
   "id": "81290795c8dce561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gc\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# path to taillard instances\n",
    "FILE_PATH = \"taillard/{nj}x{nm}\"\n",
    "\n",
    "results = {}\n",
    "instance_types = [(15, 15), (20, 15), (20, 20), (30, 15), (30, 20)]\n",
    "\n",
    "for instance_type in instance_types:\n",
    "    print(\"------------\")\n",
    "    nj, nm = instance_type\n",
    "    prepare_taillard_data(nj, nm)\n",
    "    dataset = env.dataset(batch_size=[10], phase=\"test\", filename=FILE_PATH.format(nj=nj, nm=nm))\n",
    "    dl = DataLoader(dataset, batch_size=5, collate_fn=dataset.collate_fn)\n",
    "    rewards = []\n",
    "\n",
    "    for batch in dl:\n",
    "        td = env.reset(batch).to(device)\n",
    "        # use policy.generate to avoid grad calculations which can lead to oom\n",
    "        out = model.policy.generate(td, env=env, phase=\"test\", decode_type=\"multistart_sampling\", num_starts=100, select_best=True)\n",
    "        rewards.append(out[\"reward\"])\n",
    "\n",
    "    reward = torch.cat(rewards, dim=0).mean().item()\n",
    "    results[instance_type] = reward\n",
    "\n",
    "    print(\"Done evaluating instance type %s with reward %s\" % (instance_type, reward))\n",
    "\n",
    "    # avoid ooms due to cache not being cleared\n",
    "    model.rb.empty()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "625439833e98b6e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
