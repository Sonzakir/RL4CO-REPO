{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a472f0f5-8212-443e-8e7a-41356eb24f48",
   "metadata": {},
   "source": [
    "## This is the training the model after i have defined _make_spec() method in DJSSP and after i have defined machine_breakdowns shape there."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T13:32:32.285267Z",
     "start_time": "2024-12-14T13:32:32.085296Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "from rl4co.envs.scheduling.djssp.env import DJSSPEnv\n",
    "from rl4co.models import L2DPolicy, L2DModel\n",
    "from rl4co.utils import RL4COTrainer\n",
    "import gc\n",
    "from rl4co.envs import JSSPEnv\n",
    "from rl4co.models.zoo.l2d.model import L2DPPOModel\n",
    "from rl4co.models.zoo.l2d.policy import L2DPolicy4PPO\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "generator_params = {\n",
    "\"num_jobs\" : 8 ,\n",
    "\"num_machines\": 8 ,\n",
    "\"min_processing_time\": 1 ,\n",
    "\"max_processing_time\": 99 ,\n",
    "\"mtbf\" : 17 ,\n",
    "\"mttr\" : 4\n",
    "}\n",
    "env = DJSSPEnv(generator_params=generator_params,\n",
    "_torchrl_mode=True,\n",
    "stepwise_reward=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c0ff5272ae5bd347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T13:32:36.867926Z",
     "start_time": "2024-12-14T13:32:36.685874Z"
    }
   },
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    batch_size = 4\n",
    "    train_data_size = 2_000\n",
    "    embed_dim = 128\n",
    "    num_encoder_layers = 4\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    batch_size = 1\n",
    "    train_data_size = 1_000\n",
    "    embed_dim = 64\n",
    "    num_encoder_layers = 2"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9dfa97fbc5a5cd76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T13:32:38.258031Z",
     "start_time": "2024-12-14T13:32:38.064341Z"
    }
   },
   "source": [
    "# Policy: neural network, in this case with encoder-decoder architecture\n",
    "policy = L2DPolicy4PPO(\n",
    "    embed_dim=embed_dim,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    env_name=\"djssp\",\n",
    "    het_emb=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "42d79e1a8d88b95c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T13:32:40.365217Z",
     "start_time": "2024-12-14T13:32:40.151637Z"
    }
   },
   "source": [
    "model = L2DPPOModel(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    batch_size=batch_size,\n",
    "    train_data_size=train_data_size,\n",
    "    val_data_size=1,\n",
    "    optimizer_kwargs={\"lr\": 1e-4}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "9a020a00e594b1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T13:40:37.849119Z",
     "start_time": "2024-12-14T13:32:42.269185Z"
    }
   },
   "source": [
    "# CHECKPOINT_PATH = \"last.ckpt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainer = RL4COTrainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "model = model.to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:512: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Overriding gradient_clip_val to None for 'automatic_optimization=False' models\n",
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:44: Attribute 'policy' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['policy'])`.\n",
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "\n",
      "  | Name       | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | env        | DJSSPEnv      | 0      | train\n",
      "1 | policy     | L2DPolicy4PPO | 25.5 K | train\n",
      "2 | policy_old | L2DPolicy4PPO | 25.5 K | train\n",
      "-----------------------------------------------------\n",
      "51.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "51.1 K    Total params\n",
      "0.204     Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "C:\\Users\\soner\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The batch size (256) is greater than the storage capacity (64). This makes it impossible to return a sample without repeating indices. Consider changing the sampler class or turn the 'drop_last' argument to False.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 11\u001B[0m\n\u001B[0;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m RL4COTrainer(\n\u001B[0;32m      5\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      6\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39maccelerator,\n\u001B[0;32m      7\u001B[0m     devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      8\u001B[0m     logger\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m      9\u001B[0m )\n\u001B[1;32m---> 11\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\utils\\trainer.py:146\u001B[0m, in \u001B[0;36mRL4COTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    141\u001B[0m         log\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    142\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOverriding gradient_clip_val to None for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mautomatic_optimization=False\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m models\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    143\u001B[0m         )\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_clip_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 146\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    570\u001B[0m     ckpt_path,\n\u001B[0;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m )\n\u001B[1;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1023\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[0;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1025\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1027\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[1;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:252\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    250\u001B[0m             batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautomatic_optimization\u001B[38;5;241m.\u001B[39mrun(trainer\u001B[38;5;241m.\u001B[39moptimizers[\u001B[38;5;241m0\u001B[39m], batch_idx, kwargs)\n\u001B[0;32m    251\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m             batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[0;32m    256\u001B[0m \u001B[38;5;66;03m# update non-plateau LR schedulers\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# update epoch-interval ones only when we are at the end of training epoch\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\manual.py:94\u001B[0m, in \u001B[0;36m_ManualOptimization.run\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_start()\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m suppress(\u001B[38;5;167;01mStopIteration\u001B[39;00m):  \u001B[38;5;66;03m# no loop to break at this level\u001B[39;00m\n\u001B[1;32m---> 94\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_run_end()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\manual.py:114\u001B[0m, in \u001B[0;36m_ManualOptimization.advance\u001B[1;34m(self, kwargs)\u001B[0m\n\u001B[0;32m    111\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# manually capture logged metrics\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m kwargs  \u001B[38;5;66;03m# release the batch from memory\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[1;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mtraining_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\common\\base.py:258\u001B[0m, in \u001B[0;36mRL4COLitModule.training_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: Any, batch_idx: \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    257\u001B[0m     \u001B[38;5;66;03m# To use new data every epoch, we need to call reload_dataloaders_every_epoch=True in Trainer\u001B[39;00m\n\u001B[1;32m--> 258\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mphase\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\ppo\\stepwise_ppo.py:162\u001B[0m, in \u001B[0;36mStepwisePPO.shared_step\u001B[1;34m(self, batch, batch_idx, phase, dataloader_idx)\u001B[0m\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# if iter mod x = 0 then update the policy (x = 1 in paper)\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mppo_cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate_timestep\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 162\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrb\u001B[38;5;241m.\u001B[39mempty()\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Desktop\\rl4co\\rl4co\\models\\rl\\ppo\\stepwise_ppo.py:82\u001B[0m, in \u001B[0;36mStepwisePPO.update\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# PPO inner epoch\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mppo_cfg[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mppo_epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[1;32m---> 82\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sub_td \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrb:\n\u001B[0;32m     83\u001B[0m         sub_td \u001B[38;5;241m=\u001B[39m sub_td\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     84\u001B[0m         previous_reward \u001B[38;5;241m=\u001B[39m sub_td[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:806\u001B[0m, in \u001B[0;36mReplayBuffer.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot iterate over the replay buffer. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    801\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch_size was not specified during construction of the replay buffer.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    802\u001B[0m     )\n\u001B[0;32m    803\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler\u001B[38;5;241m.\u001B[39mran_out \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m    804\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prefetch \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prefetch_queue)\n\u001B[0;32m    805\u001B[0m ):\n\u001B[1;32m--> 806\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:1293\u001B[0m, in \u001B[0;36mTensorDictReplayBuffer.sample\u001B[1;34m(self, batch_size, return_info, include_info)\u001B[0m\n\u001B[0;32m   1285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1286\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minclude_info is going to be deprecated soon.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1288\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe default behavior has changed to `include_info=True` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto avoid bugs linked to wrongly preassigned values in the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput tensordict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1291\u001B[0m     )\n\u001B[1;32m-> 1293\u001B[0m data, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1294\u001B[0m is_tc \u001B[38;5;241m=\u001B[39m is_tensor_collection(data)\n\u001B[0;32m   1295\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tc \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tensorclass(data) \u001B[38;5;129;01mand\u001B[39;00m include_info \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:715\u001B[0m, in \u001B[0;36mReplayBuffer.sample\u001B[1;34m(self, batch_size, return_info)\u001B[0m\n\u001B[0;32m    708\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    709\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size not specified. You can specify the batch_size when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    710\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconstructing the replay buffer, or pass it to the sample method. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    711\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRefer to the ReplayBuffer documentation \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    712\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor a proper usage of the batch-size arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    713\u001B[0m     )\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prefetch:\n\u001B[1;32m--> 715\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    717\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_futures_lock:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\utils.py:72\u001B[0m, in \u001B[0;36mpin_memory_output.<locals>.decorated_fun\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorated_fun\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 72\u001B[0m     output \u001B[38;5;241m=\u001B[39m fun(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m     74\u001B[0m         _tuple_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\replay_buffers.py:1326\u001B[0m, in \u001B[0;36mTensorDictReplayBuffer._sample\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m   1323\u001B[0m \u001B[38;5;129m@pin_memory_output\u001B[39m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sample\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch_size: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Any, \u001B[38;5;28mdict\u001B[39m]:\n\u001B[0;32m   1325\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replay_lock:\n\u001B[1;32m-> 1326\u001B[0m         index, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sampler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_storage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1327\u001B[0m         info[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m index\n\u001B[0;32m   1328\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_storage\u001B[38;5;241m.\u001B[39mget(index)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\env_rl4co\\lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:244\u001B[0m, in \u001B[0;36mSamplerWithoutReplacement.sample\u001B[1;34m(self, storage, batch_size)\u001B[0m\n\u001B[0;32m    242\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sample_list(storage, len_storage, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m len_storage \u001B[38;5;241m<\u001B[39m batch_size \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_last:\n\u001B[1;32m--> 244\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    245\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe batch size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) is greater than the storage capacity (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlen_storage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    246\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis makes it impossible to return a sample without repeating indices. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    247\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConsider changing the sampler class or turn the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdrop_last\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m argument to False.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    248\u001B[0m     )\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlen_storage \u001B[38;5;241m=\u001B[39m len_storage\n\u001B[0;32m    250\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_single_sample(len_storage, batch_size)\n",
      "\u001B[1;31mValueError\u001B[0m: The batch size (256) is greater than the storage capacity (64). This makes it impossible to return a sample without repeating indices. Consider changing the sampler class or turn the 'drop_last' argument to False."
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
